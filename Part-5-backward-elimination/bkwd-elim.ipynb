{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Elimintaion\n",
    "\n",
    "In the [previous post](http://javahabit.com/2019/02/02/part-4-ml-multiple-linear-regression/) we learnt about multiple linear elimination. The problem with the last approach was that we used all the features without considering that some of the features may not be impacting or playing any role in the outcome. we also talked about 5 ways of reducing the noisy feature. Backward elimination is one of them. \n",
    "\n",
    "### What is *`Backward Elimination`*?\n",
    "Backward elimination is a process to remove features that have little effect on the dependent variable.\n",
    "\n",
    "### What could possibly be wrong with leaving the features if they are not impacting or have little impact?\n",
    "`New England Patriots` won the superbowl on Feb 3, 2019. The team won becaue it had better team, better skills and good coach. If I say, the team also won because patriots fans are the great at cheering and that when patriots play fans supporting the opposition is more tamed, the I would be wrong. If I say that all players in the team wore white jersey and they won. They also won because they played it on Sunday and `T. Brady` thinks that it's his luckiesr day. You would call Baloney to all the facts that I just mentioned. It may have helped -  may be slightly but too insgignificant to make a real difference. The features in a data set are exactly that - `Baloney`. They only add noise in the actual model and many small non significant data may actually provide us model which is way off the margin. The simppler the model, the better the result. \n",
    "\n",
    "### How do we implement *`Backward elimination`*?\n",
    "In backward elimination, you take all the variables and create the algorithm. Select a significance level, then consider the predictor with *Highest P-value* and if `P-Value > Significance level` then eliminate the variable from the equation, else keep it.\n",
    "![bkwd-elim](resources\\img\\bkwdelim\\backward-elimination.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we implement it in python?\n",
    "In the [previous post](http://javahabit.com/2019/02/02/part-4-ml-multiple-linear-regression/) we were trying to figure out if a company is profitable or not by looking at 4 independent variables - `R&D Spent`, `Administration cost`, `Market spending` & `State`. We created a model with all the features. So let's pick up from where we left off. Here's how our dataset looks like\n",
    "![dataset](resources\\img\\bkwdelim\\dataset.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dines\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\dines\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:390: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.59284160e+02  6.99369053e+02  7.73467193e-01  3.28845975e-02\n",
      "  3.66100259e-02]\n",
      "42554.16761772438\n",
      "Train Score:  0.9501847627493607\n",
      "Test Score:  0.9347068473282446\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#Read the dataset\n",
    "dataset = pd.read_csv(\"50_Startups.csv\")\n",
    "\n",
    "#Divide the dataset in dependent and Independent variables\n",
    "X= dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "\n",
    "\n",
    "#Taking care of Categorical values.\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "label_encoder = LabelEncoder();\n",
    "X[:,3]=label_encoder.fit_transform(X[:, 3])\n",
    "oneHotEncoder = OneHotEncoder(categorical_features=[3])\n",
    "X= oneHotEncoder.fit_transform(X).toarray()\n",
    "\n",
    "#getting out of dummy variable trap\n",
    "X = X[:,1:] # Select all the rows and all the columns starting fom index 1 onwards.\n",
    "#Create training and test set\n",
    "from  sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.20,\n",
    "                                                    train_size=0.80,\n",
    "                                                    random_state=0)\n",
    "\n",
    "\n",
    "#Check for missing data\n",
    "null_columns=dataset.columns[dataset.isnull().any()]\n",
    "t = dataset[null_columns].isnull().sum()\n",
    "\n",
    "#Training the model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the results of the training set\n",
    "y_pred = regressor.predict(X_test)  \n",
    "\n",
    "\n",
    "print(regressor.coef_)\n",
    "print(regressor.intercept_)\n",
    "\n",
    "print('Train Score: ', regressor.score(X_train, y_train))\n",
    "print('Test Score: ', regressor.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a note of `Train Score` and `Test Score`\n",
    ">Train Score:  0.9501847627493607 \n",
    "\n",
    ">Test Score:  0.9347068473282446"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between them is `0.01547791542` or `1.548%`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we used all all the features. Now to use backward elimination we will use an entirely new package and class. However, before we begin, we need to decide on a `significance level`. In this case let's chose a level equal o **0.05**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  statsmodels.formula.api as smf\n",
    "\n",
    "#Appending ones for constants\n",
    "X = np.append(arr=np.ones((50,1)).astype(int), values=X, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why did we append 1's in the existing dataset?\n",
    ">Y = b0X0 + b1X1+ b2X2 + b3X3 + b4X4 + b5X5 + C\n",
    "\n",
    "In the above equation, if you notice that every **X<sub>n</sub>** has a multiplier **b<sub>n</sub>** but not the constant **C**. Actually if you have a **X<sub>6</sub>** and set it to 1 that solves the prblem. The question is why do we need a 1 multiplier for the constant. The answer lies in the library and class that we use. The package statsmodel only considers a multipler if it has a feature value. If there is no feature value then it would not get picked up while creating the model. So the **C** would be dropped. Hence we need a create feature with value = 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.951\n",
      "Model:                            OLS   Adj. R-squared:                  0.945\n",
      "Method:                 Least Squares   F-statistic:                     169.9\n",
      "Date:                Sun, 10 Feb 2019   Prob (F-statistic):           1.34e-27\n",
      "Time:                        22:42:26   Log-Likelihood:                -525.38\n",
      "No. Observations:                  50   AIC:                             1063.\n",
      "Df Residuals:                      44   BIC:                             1074.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       5.013e+04   6884.820      7.281      0.000    3.62e+04     6.4e+04\n",
      "x1           198.7888   3371.007      0.059      0.953   -6595.030    6992.607\n",
      "x2           -41.8870   3256.039     -0.013      0.990   -6604.003    6520.229\n",
      "x3             0.8060      0.046     17.369      0.000       0.712       0.900\n",
      "x4            -0.0270      0.052     -0.517      0.608      -0.132       0.078\n",
      "x5             0.0270      0.017      1.574      0.123      -0.008       0.062\n",
      "==============================================================================\n",
      "Omnibus:                       14.782   Durbin-Watson:                   1.283\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.266\n",
      "Skew:                          -0.948   Prob(JB):                     2.41e-05\n",
      "Kurtosis:                       5.572   Cond. No.                     1.45e+06\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.45e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "##Creating a model with all varibales\n",
    "x_opt = X[:,[0,1,2,3,4,5]]\n",
    "regressor_OLS = smf.OLS(endog=y, exog=x_opt).fit()\n",
    "print(regressor_OLS.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the process, we now have to find the feature with the highest **`P-value`** and if it greater than ould **`SL`** then we will drop it. In this case \n",
    "> x2 has the highest P-value = 0.990 > 0.05.\n",
    "\n",
    "So will drop feature x2 which corresponds to `State dummy variable` \n",
    "![allfeature](resources\\img\\bkwdelim\\featureall.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue and re run the model with just 5 feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.951\n",
      "Model:                            OLS   Adj. R-squared:                  0.946\n",
      "Method:                 Least Squares   F-statistic:                     217.2\n",
      "Date:                Sun, 10 Feb 2019   Prob (F-statistic):           8.49e-29\n",
      "Time:                        22:52:32   Log-Likelihood:                -525.38\n",
      "No. Observations:                  50   AIC:                             1061.\n",
      "Df Residuals:                      45   BIC:                             1070.\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       5.011e+04   6647.870      7.537      0.000    3.67e+04    6.35e+04\n",
      "x1           220.1585   2900.536      0.076      0.940   -5621.821    6062.138\n",
      "x2             0.8060      0.046     17.606      0.000       0.714       0.898\n",
      "x3            -0.0270      0.052     -0.523      0.604      -0.131       0.077\n",
      "x4             0.0270      0.017      1.592      0.118      -0.007       0.061\n",
      "==============================================================================\n",
      "Omnibus:                       14.758   Durbin-Watson:                   1.282\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.172\n",
      "Skew:                          -0.948   Prob(JB):                     2.53e-05\n",
      "Kurtosis:                       5.563   Cond. No.                     1.40e+06\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.4e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "### Removing index 2 as P>0.05 and is the highest P\n",
    "x_opt = X[:,[0,1,3,4,5]]\n",
    "regressor_OLS = smf.OLS(endog=y, exog=x_opt).fit()\n",
    "print(regressor_OLS.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again in the above output\n",
    "> x1 has the highest P-value = 0.940 > 0.05\n",
    "\n",
    "So we will drop X1, which in this case second dummy variable for `state`.\n",
    "![allfeature](resources\\img\\bkwdelim\\feature-5.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will continue until we dont have varibale that is greater than our **`significance level`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.951\n",
      "Model:                            OLS   Adj. R-squared:                  0.948\n",
      "Method:                 Least Squares   F-statistic:                     296.0\n",
      "Date:                Sun, 10 Feb 2019   Prob (F-statistic):           4.53e-30\n",
      "Time:                        22:59:29   Log-Likelihood:                -525.39\n",
      "No. Observations:                  50   AIC:                             1059.\n",
      "Df Residuals:                      46   BIC:                             1066.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       5.012e+04   6572.353      7.626      0.000    3.69e+04    6.34e+04\n",
      "x1             0.8057      0.045     17.846      0.000       0.715       0.897\n",
      "x2            -0.0268      0.051     -0.526      0.602      -0.130       0.076\n",
      "x3             0.0272      0.016      1.655      0.105      -0.006       0.060\n",
      "==============================================================================\n",
      "Omnibus:                       14.838   Durbin-Watson:                   1.282\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.442\n",
      "Skew:                          -0.949   Prob(JB):                     2.21e-05\n",
      "Kurtosis:                       5.586   Cond. No.                     1.40e+06\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.4e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "### Removing index 1 as P>0.05 and is the highest P\n",
    "x_opt = X[:,[0,3,4,5]]\n",
    "regressor_OLS = smf.OLS(endog=y, exog=x_opt).fit()\n",
    "print(regressor_OLS.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.950\n",
      "Model:                            OLS   Adj. R-squared:                  0.948\n",
      "Method:                 Least Squares   F-statistic:                     450.8\n",
      "Date:                Sun, 10 Feb 2019   Prob (F-statistic):           2.16e-31\n",
      "Time:                        22:59:39   Log-Likelihood:                -525.54\n",
      "No. Observations:                  50   AIC:                             1057.\n",
      "Df Residuals:                      47   BIC:                             1063.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       4.698e+04   2689.933     17.464      0.000    4.16e+04    5.24e+04\n",
      "x1             0.7966      0.041     19.266      0.000       0.713       0.880\n",
      "x2             0.0299      0.016      1.927      0.060      -0.001       0.061\n",
      "==============================================================================\n",
      "Omnibus:                       14.677   Durbin-Watson:                   1.257\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.161\n",
      "Skew:                          -0.939   Prob(JB):                     2.54e-05\n",
      "Kurtosis:                       5.575   Cond. No.                     5.32e+05\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.32e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "### Removing index 1 as P>0.05 and is the highest P\n",
    "x_opt = X[:,[0,3,5]]\n",
    "regressor_OLS = smf.OLS(endog=y, exog=x_opt).fit()\n",
    "print(regressor_OLS.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.947\n",
      "Model:                            OLS   Adj. R-squared:                  0.945\n",
      "Method:                 Least Squares   F-statistic:                     849.8\n",
      "Date:                Sun, 10 Feb 2019   Prob (F-statistic):           3.50e-32\n",
      "Time:                        22:59:43   Log-Likelihood:                -527.44\n",
      "No. Observations:                  50   AIC:                             1059.\n",
      "Df Residuals:                      48   BIC:                             1063.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       4.903e+04   2537.897     19.320      0.000    4.39e+04    5.41e+04\n",
      "x1             0.8543      0.029     29.151      0.000       0.795       0.913\n",
      "==============================================================================\n",
      "Omnibus:                       13.727   Durbin-Watson:                   1.116\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               18.536\n",
      "Skew:                          -0.911   Prob(JB):                     9.44e-05\n",
      "Kurtosis:                       5.361   Cond. No.                     1.65e+05\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.65e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "### Removing index 1 as P>0.05 and is the highest P\n",
    "x_opt = X[:,[0,3]]\n",
    "regressor_OLS = smf.OLS(endog=y, exog=x_opt).fit()\n",
    "print(regressor_OLS.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the end, we find that only the `C` constant and **`R&D Spending`** are really important or most significant feature to find out if we should invest in the new business venture.\n",
    "\n",
    "### How do I believe you that by just keeping R&D feature, will improve our model accuracy?\n",
    "Let's recalculate our model using **Linear Regression library** and find the difference between accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#Read the dataset\n",
    "dataset = pd.read_csv(\"50_Startups.csv\")\n",
    "\n",
    "#Divide the dataset in dependent and Independent variables\n",
    "X= dataset.iloc[:, 0].values ##Get the R&D score only\n",
    "y = dataset.iloc[:, -1].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training and test set\n",
    "from  sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.20,\n",
    "                                                    train_size=0.80,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8516228]\n",
      "48416.297661385026\n",
      "Train Score:  0.9449589778363044\n",
      "Test Score:  0.9464587607787219\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(np.array(X_train).reshape(-1,1), y_train)\n",
    "\n",
    "# Predicting the results of the training set\n",
    "y_pred = regressor.predict(np.array(X_test).reshape(-1,1))\n",
    "\n",
    "\n",
    "print(regressor.coef_)\n",
    "print(regressor.intercept_)\n",
    "\n",
    "print('Train Score: ', regressor.score(np.array(X_train).reshape(-1,1), y_train))\n",
    "print('Test Score: ', regressor.score(np.array(X_test).reshape(-1,1), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `Train score` and `Test Score` with all the feature and with just `R&D spending`.\n",
    "> With all Features\n",
    ">Train Score:  `0.9501847627493607`  & Test Score:  `0.9347068473282446`\n",
    "\n",
    "> Difference = `0.01547791542` or `1.548%`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With just R&D spending feature\n",
    "> Train Score: `0.9449589778363044`  & Test Score: `0.9464587607787219`\n",
    "\n",
    "> Difference = 0.0014997829424 or 0.150%\n",
    "\n",
    "Also the if you see that the test score has improved when from 93.6% to 94.6%.\n",
    "\n",
    "Hopefully, you enjoyed this series. In the next series, we will look at slighly more interesting topic called **SVMs or Support vector Regression**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
