{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [previous post](http://javahabit.com/2019/01/27/part-3-ml-understanding-p-value/), we learnt about a P-Value, a prerequiste for learning Multiple Linear Regression.\n",
    "\n",
    "**Business Problem**: In this series we will take a look at a dataset of 50 startup companies.\n",
    "![sample-data](sample-data.PNG)\n",
    "A venture capitalist has hired you as a data scientist and wants you to help him select which type of company he should invest so that he can make most profit. You need to review spending on R&D,  Admin cost, marketing cost and location to make the decision\n",
    "\n",
    "\n",
    "## How to get dataset\n",
    "- [Startup Dataset](https://github.com/dinesh19aug/ml-notes/blob/master/Part-4-multiple-regression/50_Startups.csv)\n",
    "- [Multiple Regression notebook](https://github.com/dinesh19aug/ml-notes/blob/master/Part-4-multiple-regression/multiple-regression.ipynb)\n",
    "\n",
    "## What is Multiple Linear Regression?\n",
    "In the [second post](http://javahabit.com/2019/01/22/part-2-ml-simplelinear-regression/) on linear regression our equation was simple and straight-forward\n",
    "\n",
    "> Y = mX + C\n",
    "\n",
    "where `Y` was the dependent variable, `X` was the dependent varibale and `c` was the Y-intercept when `X = 0`. The reason the equation was simple because the `Y` was dependent on one variable only. If there were multiple variable affecting the value of `Y`, then what should it be called? - :-). You know the answer to that question.\n",
    "> Multiple Linear Regression!!!\n",
    "\n",
    "The equation would also be something as simple as that\n",
    ">Y = b<sub>0</sub>X<sub>0</sub> + b<sub>1</sub>X<sub>1</sub>+b<sub>2</sub>X<sub>2</sub>+ .... + C\n",
    "\n",
    "## Steps to solve the problem?\n",
    "\n",
    "- **Step 1: Data pre-processing and analysis**\n",
    "\n",
    "Take a closer look at dataset. You will notice that all the `Independent Variables`, except `State` is numerical. `State` is either **California** or **New York**. From our [first post](https://github.com/dinesh19aug/javahabit.com/blob/master/content/posts/Part-1-Machine-learning-data-preprocessing.md), you would know that this type of data is called categorical data. We should always convert categorical data to numerical data to avoid bias and find if there is a collinearity between `Profit` and `State`. **Collinearity** is just a fancy way of asking - *\"Is there some realtion between `Profit` and `State`?*.\n",
    "When you convert a categorical data to numeric data, the new column is called **Dummy Variable**. So as we learned from our [first post](https://github.com/dinesh19aug/javahabit.com/blob/master/content/posts/Part-1-Machine-learning-data-preprocessing.md), we should convert it to sparse matrix.\n",
    "![dummy](dummy1.PNG)\n",
    "\n",
    "**Question**\n",
    "> *Do you need two columns to represent **New York** and **California** states?\n",
    "The answer is **No**.\n",
    "\n",
    "![dummy2](dummy2.PNG)\n",
    "\n",
    "It is easy to derive from the above screenshot that if **New York** is 1 then **California** by default would be 0 and vice-versa. So this actaully works like an switch which can have only 2 states **0** or **1**. \n",
    "> Important tip: You should never use all of your dummy variables in your Regression column. They should always be **1** less than the number of values.\n",
    "\n",
    "**If we drop one dummy variable then are we not making this a biased equation**\n",
    "\n",
    "Without dropping column my equation would look like this\n",
    "> Y = b<sub>0</sub>X<sub>0</sub> + b<sub>1</sub>X<sub>1</sub>+ b<sub>2</sub>X<sub>2</sub> + b<sub>3</sub>X<sub>3</sub> + b<sub>4</sub>X<sub>4</sub> + b<sub>5</sub>X<sub>5</sub> + C\n",
    "\n",
    "After Dropping one dummy variable\n",
    "> > Y = b<sub>0</sub>X<sub>0</sub> + b<sub>1</sub>X<sub>1</sub>+ b<sub>2</sub>X<sub>2</sub> + b<sub>3</sub>X<sub>3</sub> + b<sub>4</sub>X<sub>4</sub> + C\n",
    "\n",
    "If we dropped the variable then it may appear that when `California` is the state then value of **b<sub>4</sub>X<sub>4</sub>** will be 0, hence we loose one variable. In reality the regression algorithm, marks that the first dummy variable which is represented by `0` is set as default. So, the regression equation wil never be biased. The regression equation is going to use constant `C` to adjust the value of `California`.\n",
    "\n",
    "**But what is wrong with using all the dummy variable?**\n",
    "\n",
    "When you use both the values, your equation would be something\n",
    "> Y = b<sub>0</sub>X<sub>0</sub> + b<sub>1</sub>X<sub>1</sub>+ b<sub>2</sub>X<sub>2</sub> + b<sub>3</sub>X<sub>3</sub> + b<sub>4</sub>X<sub>4</sub> + b<sub>5</sub>X<sub>5</sub> + C\n",
    "\n",
    "If we do this then we will introduce multi-collinearity, where algorithm will not be able to able to distinguish the effect on `Price`. This is because **D<sub>2</sub>** is always equal to **1 - D<sub>1</sub>**. The algorithm will then try to prodict the effect of `D2` over `D1` and would think that there is relation between `Independent` variable as well. \n",
    "\n",
    "![dummy3](dummy3.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 2: Understanding all possible methods to build model using Multiple Linear Regression model**\n",
    "\n",
    "There are 5 methods to build a model\n",
    "    - All in one\n",
    "    - Backward Elimination\n",
    "    - Forward Selection\n",
    "    - Bi-Directional Elimination\n",
    "    - Score comparison\n",
    "    \n",
    " > **All In** : All in means that you use all the variables when you know for sure that all the independent variables have a definite effect on dependent variable. An example is that if doctors told you for sure that to live past 80 yrs of age, you should eat good food and exercise daily. In other words, you have domain expert telling you that all the variables directly affect the dependent variable.\n",
    " \n",
    " > **Backward elimintaion** : In backward elimination, you take all the variables and create the algorithm. Select a significance level, then consider the predictor with *Highest P-value* and if `P-Value > Significance level` then eliminate the varibale from the equation, else keep it.\n",
    "![backward-elimination](backward-elimination.PNG)\n",
    "\n",
    " > **Forward Selection** : In forward selection, you start with linear regression using every single variable. You will end up with `n` simple linear equation. Next you chose the one with the lowest P-Value. This is your starting equation. **Y = m<sub>0</sub>X<sub>0</sub>**. Next you pick one variable again and create a equation with two varibale and out of `n-1` possibilities again chose the one with the lowest P-value. The process continues until we dont have any varibale that is lower than our selected Significance level.\n",
    " ![fwd-selection.PNG](fwd-selection.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  > **Bi-Directional elimination**: In bi-directional elimination, you chose 2 significance level. One to enter the equation and one to stay. You start with the forward selection using condition **P-Value < SL <sub>Enter</sub>** and then follow backward elimination using condition **P-value < SL<sub>stay</sub>**. You stop and declare the final equation when no new variable can enter or exit the equation.\n",
    " ![bidirection](bidirection.PNG)\n",
    " \n",
    " > **Score Comparison**: In this model you create all posible combination of equation, compare the performance using say MSE(Mean Square error) and use the one with the lowest MSE. That is an insane amount of possible equation. For Example - A model with 10 variables will have 1023 possible combination.\n",
    " ![all-possible.PNG](all-possible.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Note for the purpose of brevity and sanity, we will be using `Backward Elimitaion` model to solve this problem. Also, because this model is fastest and we will still be able to see how the step by method works.**\n",
    " \n",
    "- **Step 3: Datapreprocessing in python**\n",
    "\n",
    "We will now start the calculation using the process that we learnt in [first post of this series](http://javahabit.com/2019/01/21/part-1-ml-data-preprocessing/). If you remeber from Linear regression post\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as py\n",
    "import pandas as pd\n",
    "\n",
    "#Read the dataset\n",
    "dataset = pd.read_csv(\"50_Startups.csv\")\n",
    "\n",
    "#Divide the dataset in dependent and Independent variables\n",
    "X= dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataset.PNG](dataset.PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next part of data processing is to find if there is any missing data. If there is missing data then we need to use **Imputer** to fix the missing data. To do that we will first check the data desription, then try to get a count of missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "#Describe the dataset\n",
    "pd.DataFrame.describe(dataset)\n",
    "#Check for missing data\n",
    "null_columns=dataset.columns[dataset.isnull().any()]\n",
    "num_emptycolumns = dataset[null_columns].isnull().sum()\n",
    "print(num_emptycolumns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, there are no missing values in the dataset hence the result of **`num_emptycolumns`**.\n",
    "\n",
    "Moving on to next item in the data clean up is taking care of categorical values in the **`State`** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dines\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\dines\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:390: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Taking care of Categorical values.\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "label_encoder = LabelEncoder();\n",
    "X[:,3]=label_encoder.fit_transform(X[:, 3])\n",
    "oneHotEncoder = OneHotEncoder(categorical_features=[3])\n",
    "X= oneHotEncoder.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![encoding.PNG](encoding.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Hey what about all the talk about keeping n-1 categorical items?**\n",
    "\n",
    "So you are right to notice that whatever we did wll lead us directly to the dummy variable trap. I fell into one when I was trying to learn it. So how do we fix it? If you have been thinking about just crossing out one of the column as we did in the pic few scrolls above ....... you are right!!! That's exactly how we are going to fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting out of dummy variable trap\n",
    "X = X[:,1:] # Select all the rows and all the columns starting fom index 1 onwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now divide the data in training and test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training and test set\n",
    "from  sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.20,\n",
    "                                                    train_size=0.80,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Training the model**\n",
    "\n",
    "Now that we have cleaned up our data, we can train our data. We are not going to do any feature scaling here, because the library and class that we are going to use is going to do that automatically for us. We will use same **`LineaRegression`** class that we used in [linear regression post](http://javahabit.com/2019/01/22/part-2-ml-simplelinear-regression/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bove code trained our data. Now we need to check how does our model score or how good is it at predicting the data?\n",
    "To Predict we just need to call the predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![predicted-res.PNG](predicted-res.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the predicted values **`y-pred`** is pretty close to **`y_test`**. Some of the rows do have significant difference and the numbers may look far apart but the others are pretty close. So how close are we? How confident are we that the trendline would fits closely. To answer this we need to look at our training and test score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:  0.9501847627493607\n",
      "Test Score:  0.9347068473282446\n"
     ]
    }
   ],
   "source": [
    "print('Train Score: ', regressor.score(X_train, y_train))\n",
    "print('Test Score: ', regressor.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above score tells us that our model has **`95%`** and `93.5%` accurate for tarining and test data. This not bad. \n",
    "One last thing that I learnt was that if someone asked me how do I calculate the future values that may come up? To do that we need to get the co-effcient and intercept of the regression equation. The regression equation as dicussed earlier would look something like this - \n",
    "> Y = b<sub>0</sub>X<sub>0</sub> + b<sub>1</sub>X<sub>1</sub>+ b<sub>2</sub>X<sub>2</sub> + b<sub>3</sub>X<sub>3</sub> + b<sub>4</sub>X<sub>4</sub> + b<sub>5</sub>X<sub>5</sub> + C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.59284160e+02  6.99369053e+02  7.73467193e-01  3.28845975e-02\n",
      "  3.66100259e-02]\n",
      "42554.16761772438\n"
     ]
    }
   ],
   "source": [
    "print(regressor.coef_)\n",
    "print(regressor.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take the first row from `X_test` - 1\t0\t66051.5\t182646\t118148\n",
    "> 1*-9.59284160e+02 + 0*6.99369053e+02 + 66051.5 * 7.73467193e-01 + 182646*3.28845975e-02 + 118148 * 3.66100259e-02 + 42554.16761772438\n",
    "\n",
    "The output would be **`103015.19329118208`**, which is same as first row of **`y_pred`**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103015.19329118208"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 * -9.59284160e+02 + 0 * 6.99369053e+02 + 66051.5 * 7.73467193e-01 + 182646 * 3.28845975e-02 + 118148 * 3.66100259e-02 + 42554.16761772438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that is the most basic way to do to solve a problem using Multiple Linear regression. Hoep you enjoyed this series. Stay tuned for the next series where we will actually see the continuation of **`Multiple Linear regression`** and learn the **Backward Elimination process**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
